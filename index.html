<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<link rel="stylesheet" href="styles.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>IDTransformer: Transformer for Intrinsic Image Decomposition</title>
    <meta property="og:description" content="IDTransformer: Transformer for Intrinsic Image Decomposition"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141699104-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141699104-1');
    </script>
</head>


<body>
<div class="container">
    <div class="paper-title">
         <h1>IDTransformer: Transformer for Intrinsic Image Decomposition</h1>
    </div>
    
    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://scholar.google.com/citations?user=4c_gDYEAAAAJ&hl=en">Partha Das</a><sup>1,3</sup></div>
            <div class="col-3 text-center">Maxime Gevers<sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://karaoglusezer.github.io/">Sezer Karaoglu</a><sup>1,3</sup></div>
            <div class="col-3 text-center"><a href="https://staff.fnwi.uva.nl/th.gevers/">Theo Gevers</a><sup>1,3</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-2 text-center"><sup>1</sup><a href="https://ivi.fnwi.uva.nl/cv/">Computer Vision Lab,<br>
                University of Amsterdam</a></div>
            <div class="col-2 text-center"><sup>2</sup>Concordia University</div>
            <div class="col-2 text-center"><sup>3</sup><a href="https://3duniversum.com/">3DUniversum</a></div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2022</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="">
                <span class="material-icons"> description </span> 
                 Paper (Coming Soon)
            </a>
            <a class="supp-btn" href="https://github.com/Morpheus3000/IDTransformer">
                <span class="material-icons"> description </span> 
                 GitHub
            </a>
            <a class="supp-btn" href="assets/bibtex.txt">
                <span class="material-icons"> description </span> 
                 BibTeX
            </a>
            <!-- <a class="supp-btn" href="assets/PIE_NET_CVPR_2022_main_paper.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/PIE_NET_CVPR_2022_supp.pdf">
                <span class="material-icons"> description </span> 
                 Supp PDF
            </a>
            <a class="supp-btn" href="https://github.com/Morpheus3000/PIE-Net">
                <span class="material-icons"> description </span> 
                 GitHub
            </a>
            <a class="supp-btn" href="assets/bibtex.txt">
                <span class="material-icons"> description </span> 
                 BibTeX
            </a> -->
            </div>
        </div>
    </div>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <p>
            The aim of intrinsic image decomposition (IID) is to recover reflectance and the shading from a given image. As different combinations are possible, IID is an under constrained problem. Previous approaches try to constrain the search space using hand crafted priors. However, these priors are based on strong imaging assumptions and fall short when these do not hold. Deep learning based methods learn the problem end-to-end from the data. But these networks lack any explicit information about the image formation model.
            <br>
            In this paper, an IID transformer approach (IDTransformer) is proposed by learning photometric invariant attention, derived from the image formation model, integrated in the transformer framework. The combination of invariant features in both a global and local setting allows the network to not only learn reflectance transitions, but also to group similar reflectance regions, irrespective of the spatial arrangement. Illumination and geometry invariant attention is exploited to generate the reflectance map, while illumination invariant and geometry variant attention is used to compute the shading map. 
            <br>           
            Enabling physics-based explicit attention allows the network to be trained on a relatively small dataset. Ablation studies show that adding invariant attention improves the performance. Experiments on the Intrinsic In the Wild dataset shows competitive results with competing methods. 
        </p>

        <!-- <figure style="width: 100%;">
            <a>
                <img width="100%" src="images/Output_teaser.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                Our model exploits <strong>illumination invariant features in an edge-driven hybrid CNN approach</strong>. The model is able to predict physically consistent reflectance and shading from a single input image, without the need for any specific priors. The network is trained without any specialised dataset or losses. 
            </p>
        </figure>

        <figure style="width: 100%; float: center">
            <a>
                <img width="90%" src="images/net_overview.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                Overview of the proposed Network. The architecture consists of 4 sub-modules. Inputs to the network are (a) <strong><i>RGB</i> image</strong> and (b) <strong>the <i>CCR</i> image</strong>. The CCR image is computed from (a). The outputs of the networks are: (c) <strong>reflectance edge</strong>, (d) <strong>shading edge</strong>, (e) <strong>the unrefined reflectance prediction</strong>, (f) <strong>the unrefined shading prediction</strong>, (g) <strong>final refined reflectance</strong>, (h) <strong>the final refined shading</strong>, and (i) <strong>scaled edge outputs @(64, 128)</strong>.
            </p>
        </figure> -->

    </section>

    <!-- <section id="results">
        <h2>Results</h2>
        <hr>
        <hr>
        <figure style="width: 100%;">
            <a>
                <img width="50%" height="35%" src="images/MIT_Comp.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            Visuals from the <strong>MIT Intrinsic test set</strong>. It is shown that the proposed algorithm predictions are <strong>closer to the ground truth IID components</strong>. IntrinsicNet, on the other hand, completely misses the shadow on the paper, while the proposed algorithm can transfer it to the shading image correctly. The deer and turtle show the proposed algorithm able to <strong>properly disentangle geometric patterns from reflectance</strong>, which are much flatter. 
            </p>
        </figure>
        <figure style="width: 100%; float: center">
            <a>
                <img width="50%" height="50%" src="images/Sintel_Comp.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            Results on the <strong>Sintel test set</strong>. It is shown that our network can remove the <strong>cast shadows</strong>, even from complex scenes like a forest. The outputs show that the reflectance is free from the cast shadows on the wall, while the shading image is <strong>free from the textures</strong> on the wooden box.
            </p>
        </figure>
        <figure style="width: 100%;">
            <a>
                <img width="50%" height="35%" src="images/IIW_Comp.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            Results of the proposed method on the <strong>In the Wild dataset</strong>. Despite being <strong>trained primarily on outdoor garden images</strong>. The proposed method obtains the reflectance that is free of hard negative illumination transitions. In the second example, the bedside light (red box) is well preserved and assigned to the shading component. The small objects (green box) on the bedside table are more distinctive compared to other methods, while additional details underneath the table (blue box) are assigned to the reflectance component.
            </p>
        </figure>
        <figure style="width: 100%;">
            <a>
                <img width="50%" height="50%" src="images/TrimbotComp.png" class="centered">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
            Visuals of the proposed method on the  <strong>Trimbot dataset</strong>. The proposed method is <strong>trained and finetuned on a fully synthetic dataset</strong>, yet it can recover proper reflectance by removing both soft and hard illumination patterns, while obtaining a smooth shading that is free from shadow-reflectance misclassifications.
            </p>
        </figure>
        
        

    </section> -->

    

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{dasIDTransformer,
                title = {IDTransformer: Transformer for Intrinsic Image Decomposition}, 
                author = {Partha Das and Maxime Gevers and Sezer Karaoglu and Theo Gevers},
                        booktitle = {New Ideas in Vision Transformers (ICCV)},
                year = {2023}
               }
</code></pre>
    </section>

    <!-- <section id="models">
        <h2>Pretrained Models</h2>
        <hr>
        <p>The pretrained models are provided at the following location:</p>
        <p class="model-links">Pretrained on synthetic outdoor data.</p>
        <p class="model-links">Finetuned on the MIT Dataset.</p>
        <p class="model-links">Finetuned on the IIW Dataset.</p>
        </table>
    </section> -->
    
    <!-- <section id="paper">
        <h2>Paper (Coming Soon)</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/PIE_NET_CVPR_2022_v2_0_main_paper.pdf"><img class="screenshot" src="images/preview.png"></a>
            </div>
            <div style="width: 55%">
                <p><bb>PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition</b></p>
                <p>Partha Das, Sezer Karaoglu, Theo Gevers</p>

                <div><span class="material-icons"> description </span><a href="assets/PIE_NET_CVPR_2022_main_paper.pdf">Paper</a></div>
                <div><span class="material-icons"> description </span><a href="assets/PIE_NET_CVPR_2022_supp.pdf">Supp PDF</a></div> 
                <!-- <div><span class="material-icons"> description </span><a href="http://arxiv.org/abs/2109.06061"> arXiv </a></div> -->
                <div><span class="material-icons"> description </span><a href="https://github.com/Morpheus3000/PIE-Net">GitHub</a></div> 
                <div><span class="material-icons"> insert_comment </span><a href="assets/bibtex.txt"> BibTeX</a></div>
            </div>
        </div>
    </section> -->

</div>
</body>
</html>
